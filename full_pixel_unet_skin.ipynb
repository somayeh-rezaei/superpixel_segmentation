{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:15:01.002500242Z",
     "start_time": "2024-02-06T10:14:57.487329171Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:14:57.478354: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-06 11:14:57.752356: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-06 11:14:58.919930: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-06 11:14:58.920196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-06 11:14:58.920205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "from keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# image_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/ISIC2018_SKIN_DATA/ISIC2018_Task1-2_Training_Input/*.png\")\n",
    "# mask_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/ISIC2018_SKIN_DATA/ISIC2018_Task1_Training_GroundTruth/*.png\")\n",
    "image_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/archive (2)/Dataset_BUSI_with_GT/breast_train/*.png\")\n",
    "mask_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/archive (2)/Dataset_BUSI_with_GT/breast_train_mask/*.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:32.762308086Z",
     "start_time": "2024-02-06T10:29:32.712990636Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# initialize empty arrays to hold the images and masks\n",
    "images = []\n",
    "masks = []\n",
    "images_superpixel = []\n",
    "binary_masks = []\n",
    "masks_superpixel = []\n",
    "masks_result = []\n",
    "binary = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:35.325547387Z",
     "start_time": "2024-02-06T10:29:35.318057349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "threshold_value = 128\n",
    "def data_generator(image_paths, mask_paths, batch_size):\n",
    "\n",
    "    assert len(image_paths) == len(mask_paths), \"Number of images and masks must be the same.\"\n",
    "\n",
    "    num_samples = len(image_paths)\n",
    "    print(\"Number of images:\", num_samples)\n",
    "    num_masks = len(mask_paths)\n",
    "    print(\"Number of masks:\", num_masks)\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i : i + batch_size]\n",
    "            batch_images = []\n",
    "            batch_masks = []\n",
    "\n",
    "            for index in batch_indices:\n",
    "                image_path = image_paths[index]\n",
    "                mask_path = mask_paths[index]\n",
    "\n",
    "                img = cv2.imread(image_path)\n",
    "                img = cv2.resize(img, (256, 256))\n",
    "\n",
    "                mask = cv2.imread(mask_path,0)\n",
    "                mask = cv2.resize(mask, (256, 256))\n",
    "\n",
    "            \n",
    "                # normalize the image and mask to have values between 0 and 1\n",
    "                img = img / 255.0\n",
    "                mask= mask/ 255.0\n",
    "\n",
    "\n",
    "                batch_images.append(img)\n",
    "                batch_masks.append(mask)\n",
    "\n",
    "            # Yield the batch data\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_masks = np.array(batch_masks)\n",
    "\n",
    "\n",
    "            yield batch_images, batch_masks\n",
    "# #\n",
    "# generator = data_generator(image_paths, mask_paths, 1)\n",
    "# \n",
    "# for _,imgpath in tqdm(enumerate(image_paths)):\n",
    "#    batch = next(generator)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:38.643316877Z",
     "start_time": "2024-02-06T10:29:38.628541452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection\n",
    "    iou_score = (intersection + smooth) / (union + smooth)\n",
    "    return iou_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:41.049341119Z",
     "start_time": "2024-02-06T10:29:41.030629543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    y_true_flat = K.flatten(y_true)\n",
    "    y_pred_flat = K.flatten(y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true_flat * y_pred_flat, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred_flat, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "    return precision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:41.503796128Z",
     "start_time": "2024-02-06T10:29:41.487325816Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def sensitivity(gt_mask, pred_mask):\n",
    "\n",
    "    gt_mask_flat = K.flatten(gt_mask)\n",
    "    pred_mask_flat = K.flatten(pred_mask)\n",
    "\n",
    "    # compute true positive (TP) and false negative (FN) counts\n",
    "    TP = K.sum(gt_mask_flat * pred_mask_flat)\n",
    "    FN = K.sum(gt_mask_flat * (1 - pred_mask_flat))\n",
    "\n",
    "    # compute sensitivity (recall)\n",
    "    if TP + FN == 0:\n",
    "        sensitivity_score = 0.0\n",
    "    else:\n",
    "        sensitivity_score = TP / (TP + FN)\n",
    "\n",
    "    return sensitivity_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:42.039138293Z",
     "start_time": "2024-02-06T10:29:42.020879998Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "      y_true_f = K.flatten(y_true)\n",
    "      y_pred_f = K.flatten(y_pred)\n",
    "      intersection = K.sum(y_true_f * y_pred_f)\n",
    "      return (2.0 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:43.775685300Z",
     "start_time": "2024-02-06T10:29:43.757297731Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred,1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:29:44.847839351Z",
     "start_time": "2024-02-06T10:29:44.819635065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "assert len(image_paths) == len(mask_paths), \"Number of images and masks must be the same.\"\n",
    "# Split the data into training and validation sets\n",
    "train_image_paths, val_image_paths, train_mask_paths, val_mask_paths = train_test_split(\n",
    "    image_paths, mask_paths, test_size=0.2, random_state=2023\n",
    ")\n",
    "\n",
    "# Create separate generators for training and validation sets\n",
    "train_generator = data_generator(train_image_paths, train_mask_paths, batch_size)\n",
    "val_generator = data_generator(val_image_paths, val_mask_paths, batch_size)\n",
    "\n",
    "# Determine the number of steps per epoch for training and validation\n",
    "train_steps_per_epoch = len(train_image_paths) // batch_size\n",
    "val_steps_per_epoch = len(val_image_paths) // batch_size\n",
    "\n",
    "print(train_steps_per_epoch)\n",
    "print(val_steps_per_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:30:05.138629635Z",
     "start_time": "2024-02-06T10:30:05.095920980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def encoder_block(inputs, num_filters): \n",
    "  \n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters,  \n",
    "                               3,  \n",
    "                               padding = 'valid')(inputs) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "      \n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters,  \n",
    "                               3,  \n",
    "                               padding = 'valid')(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "  \n",
    "    # Max Pooling with 2x2 filter \n",
    "    x = tf.keras.layers.MaxPool2D(pool_size = (2, 2), \n",
    "                                  strides = 2)(x) \n",
    "      \n",
    "    return x\n",
    "def decoder_block(inputs, skip_features, num_filters): \n",
    "  \n",
    "    # Upsampling with 2x2 filter \n",
    "    x = tf.keras.layers.Conv2DTranspose(num_filters, \n",
    "                                        (2, 2),  \n",
    "                                        strides = 2,  \n",
    "                                        padding = 'valid')(inputs) \n",
    "      \n",
    "    # Copy and crop the skip features  \n",
    "    # to match the shape of the upsampled input \n",
    "    skip_features = tf.image.resize(skip_features, \n",
    "                                    size = (x.shape[1], \n",
    "                                            x.shape[2])) \n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features]) \n",
    "      \n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters, \n",
    "                               3,  \n",
    "                               padding = 'valid')(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "  \n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding = 'valid')(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "      \n",
    "    return x\n",
    "\n",
    "def unet_model(input_shape = (572, 572, 3), num_classes = 1): \n",
    "    inputs = tf.keras.layers.Input(input_shape) \n",
    "      \n",
    "    # Contracting Path \n",
    "    s1 = encoder_block(inputs, 64) \n",
    "    s2 = encoder_block(s1, 128) \n",
    "    s3 = encoder_block(s2, 256) \n",
    "    s4 = encoder_block(s3, 512) \n",
    "      \n",
    "    # Bottleneck \n",
    "    b1 = tf.keras.layers.Conv2D(1024, 3, padding = 'valid')(s4) \n",
    "    b1 = tf.keras.layers.Activation('relu')(b1) \n",
    "    b1 = tf.keras.layers.Conv2D(1024, 3, padding = 'valid')(b1) \n",
    "    b1 = tf.keras.layers.Activation('relu')(b1) \n",
    "      \n",
    "    # Expansive Path \n",
    "    s5 = decoder_block(b1, s4, 512) \n",
    "    s6 = decoder_block(s5, s3, 256) \n",
    "    s7 = decoder_block(s6, s2, 128) \n",
    "    s8 = decoder_block(s7, s1, 64) \n",
    "      \n",
    "    # Output \n",
    "    outputs = tf.keras.layers.Conv2D(num_classes,  \n",
    "                                     1,  \n",
    "                                     padding = 'valid',  \n",
    "                                     activation = 'sigmoid')(s8) \n",
    "      \n",
    "    model = tf.keras.models.Model(inputs = inputs,  \n",
    "                                  outputs = outputs,  \n",
    "                                  name = 'U-Net') \n",
    "\n",
    "    \n",
    "    return model "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:30:24.893171123Z",
     "start_time": "2024-02-06T10:30:24.873059737Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def build_unet(input_size=(256,256,3)):\n",
    "\n",
    "    inputs  = Input(input_size)\n",
    "\n",
    "    conv1 = Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3,3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3,3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3,3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "\n",
    "    conv6 = Conv2D(1024, (3, 3), activation='relu', padding='same') (pool5)\n",
    "    conv6 = Conv2D(1024, (3, 3), activation = 'relu', padding='same')(conv6)\n",
    "    pool6 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
    "\n",
    "\n",
    "    up7 = layers.concatenate([Conv2DTranspose(512, (2,2), strides=(2,2), padding='same')(conv6), conv5], axis=3)\n",
    "\n",
    "\n",
    "    conv7 = Conv2D(512, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "\n",
    "    up8 = layers.concatenate([Conv2DTranspose(256, (2,2), strides=(2,2), padding='same')(conv7), conv4], axis=3)\n",
    "\n",
    "    conv8 = Conv2D(256, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = layers.concatenate([Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(conv8), conv3], axis=3)\n",
    "\n",
    "    conv9 = Conv2D(128, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    up10 = layers.concatenate([Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(conv9), conv2], axis=3)\n",
    "\n",
    "    conv10 = Conv2D(64, (3, 3), activation='relu', padding='same')(up10)\n",
    "    conv10 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv10)\n",
    "\n",
    "    up11 = layers.concatenate([Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(conv10), conv1], axis=3)\n",
    "\n",
    "    conv11 = Conv2D(32, (3, 3), activation='relu', padding='same')(up11)\n",
    "    conv11 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv11)\n",
    "\n",
    "    conv12 = Conv2D(1, (1,1), activation='sigmoid')(conv11)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[conv12])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:30:27.970608387Z",
     "start_time": "2024-02-06T10:30:27.926289522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "log_folder = '/home/somayeh/PycharmProjects/superpixel_segmentation/tensor_board'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:30:30.646901563Z",
     "start_time": "2024-02-06T10:30:30.640634732Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 11:30:44.829932: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2024-02-06 11:30:44.829995: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2024-02-06 11:30:45.002317: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2024-02-06 11:30:45.002568: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "# weight_path = \"{}_full_pixel_unet_skin.best.hdf5\".format('cxr_reg')\n",
    "weight_path = \"{}_full_pixel_unet_breast.best.hdf5\".format('cxr_reg')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                   patience=3,\n",
    "                                   verbose=1, mode='min', epsilon=0.0001, cooldown=2, min_lr=1e-5)\n",
    "early = EarlyStopping(monitor=\"val_loss\",\n",
    "                      mode=\"min\",\n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "\n",
    "tbcallback = TensorBoard(log_dir=log_folder,\n",
    "                         histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True,\n",
    "                         update_freq='epoch',\n",
    "                         profile_batch=2,\n",
    "                         embeddings_freq=1)\n",
    "\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat, tbcallback]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:30:45.037461196Z",
     "start_time": "2024-02-06T10:30:44.831100848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 256, 256, 32  896         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 256, 256, 32  9248        ['conv2d_23[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 128, 128, 32  0          ['conv2d_24[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 128, 128, 64  18496       ['max_pooling2d_6[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 128, 128, 64  36928       ['conv2d_25[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 64)  0           ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 64, 64, 128)  73856       ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 64, 64, 128)  147584      ['conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 128)  0          ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 32, 32, 256)  295168      ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 32, 32, 256)  590080      ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (None, 16, 16, 256)  0          ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 512)  1180160     ['max_pooling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 512)  2359808     ['conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 8, 8, 512)   0           ['conv2d_32[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 1024)   4719616     ['max_pooling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 1024)   9438208     ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 16, 16, 512)  2097664    ['conv2d_34[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 16, 16, 1024  0           ['conv2d_transpose_5[0][0]',     \n",
      "                                )                                 'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 16, 16, 512)  4719104     ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 16, 16, 512)  2359808     ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 32, 32, 256)  524544     ['conv2d_36[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 512)  0           ['conv2d_transpose_6[0][0]',     \n",
      "                                                                  'conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 32, 32, 256)  1179904     ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 32, 32, 256)  590080      ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 64, 64, 128)  131200     ['conv2d_38[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 64, 64, 256)  0           ['conv2d_transpose_7[0][0]',     \n",
      "                                                                  'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 64, 64, 128)  295040      ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 64, 64, 128)  147584      ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 128, 128, 64  32832      ['conv2d_40[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 128, 128, 12  0           ['conv2d_transpose_8[0][0]',     \n",
      "                                8)                                'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 128, 128, 64  73792       ['concatenate_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 128, 128, 64  36928       ['conv2d_41[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2DTran  (None, 256, 256, 32  8224       ['conv2d_42[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 256, 256, 64  0           ['conv2d_transpose_9[0][0]',     \n",
      "                                )                                 'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 256, 256, 32  18464       ['concatenate_9[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 256, 256, 32  9248        ['conv2d_43[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 256, 256, 1)  33          ['conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,094,497\n",
      "Trainable params: 31,094,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model =build_unet(input_size=(256, 256, 3))\n",
    "# model = unet_model(input_shape=(256, 256, 3), num_classes=1)\n",
    "opt = tf.keras.optimizers.experimental.AdamW(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss=dice_coef_loss, metrics= [dice_coef, precision, sensitivity, iou],  run_eagerly=True)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:31:06.293654761Z",
     "start_time": "2024-02-06T10:31:05.780979155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 444\n",
      "Number of masks: 444\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6106/2762390889.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  loss_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/55 [..............................] - ETA: 6:20 - loss: 0.8072 - dice_coef: 0.1928 - precision: 0.1179 - sensitivity: 0.5080 - iou: 0.1067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 12:24:10.044267: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2024-01-26 12:24:10.044315: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/55 [>.............................] - ETA: 41s - loss: 0.7848 - dice_coef: 0.2152 - precision: 0.1366 - sensitivity: 0.5074 - iou: 0.1208 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 12:24:10.726724: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2024-01-26 12:24:10.748366: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2024-01-26 12:24:10.803394: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 1073 callback api events and 1081 activity events. \n",
      "2024-01-26 12:24:10.817993: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2024-01-26 12:24:10.820396: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: /home/somayeh/PycharmProjects/superpixel_segmentation/tensor_board/plugins/profile/2024_01_26_12_24_10/somayeh-ASUS-TUF-Gaming-F15-FX506HCB-FX506HCB.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - ETA: 0s - loss: 0.8460 - dice_coef: 0.1540 - precision: 0.0992 - sensitivity: 0.5088 - iou: 0.0840Number of images: 111\n",
      "Number of masks: 111\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.80784, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 29s 398ms/step - loss: 0.8460 - dice_coef: 0.1540 - precision: 0.0992 - sensitivity: 0.5088 - iou: 0.0840 - val_loss: 0.8078 - val_dice_coef: 0.1922 - val_precision: 0.1446 - val_sensitivity: 0.5270 - val_iou: 0.1071 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.8040 - dice_coef: 0.1958 - precision: 0.1561 - sensitivity: 0.8213 - iou: 0.1097\n",
      "Epoch 2: val_loss improved from 0.80784 to 0.60398, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 374ms/step - loss: 0.8040 - dice_coef: 0.1958 - precision: 0.1561 - sensitivity: 0.8213 - iou: 0.1097 - val_loss: 0.6040 - val_dice_coef: 0.3960 - val_precision: 0.4652 - val_sensitivity: 0.5337 - val_iou: 0.2514 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.6389 - dice_coef: 0.3597 - precision: 0.3278 - sensitivity: 0.6709 - iou: 0.2288\n",
      "Epoch 3: val_loss improved from 0.60398 to 0.47844, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 369ms/step - loss: 0.6389 - dice_coef: 0.3597 - precision: 0.3278 - sensitivity: 0.6709 - iou: 0.2288 - val_loss: 0.4784 - val_dice_coef: 0.5216 - val_precision: 0.6679 - val_sensitivity: 0.4323 - val_iou: 0.3573 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.5001 - dice_coef: 0.5010 - precision: 0.5470 - sensitivity: 0.5188 - iou: 0.3426\n",
      "Epoch 4: val_loss improved from 0.47844 to 0.43311, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 372ms/step - loss: 0.5001 - dice_coef: 0.5010 - precision: 0.5470 - sensitivity: 0.5188 - iou: 0.3426 - val_loss: 0.4331 - val_dice_coef: 0.5669 - val_precision: 0.6463 - val_sensitivity: 0.5361 - val_iou: 0.4059 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.4484 - dice_coef: 0.5520 - precision: 0.5831 - sensitivity: 0.5842 - iou: 0.3884\n",
      "Epoch 5: val_loss improved from 0.43311 to 0.43015, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 370ms/step - loss: 0.4484 - dice_coef: 0.5520 - precision: 0.5831 - sensitivity: 0.5842 - iou: 0.3884 - val_loss: 0.4301 - val_dice_coef: 0.5699 - val_precision: 0.7703 - val_sensitivity: 0.4665 - val_iou: 0.4098 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.4403 - dice_coef: 0.5598 - precision: 0.5903 - sensitivity: 0.5815 - iou: 0.3975\n",
      "Epoch 6: val_loss improved from 0.43015 to 0.39694, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 371ms/step - loss: 0.4403 - dice_coef: 0.5598 - precision: 0.5903 - sensitivity: 0.5815 - iou: 0.3975 - val_loss: 0.3969 - val_dice_coef: 0.6031 - val_precision: 0.7403 - val_sensitivity: 0.5255 - val_iou: 0.4383 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.4291 - dice_coef: 0.5725 - precision: 0.5889 - sensitivity: 0.6262 - iou: 0.4160\n",
      "Epoch 7: val_loss improved from 0.39694 to 0.37836, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 368ms/step - loss: 0.4291 - dice_coef: 0.5725 - precision: 0.5889 - sensitivity: 0.6262 - iou: 0.4160 - val_loss: 0.3784 - val_dice_coef: 0.6216 - val_precision: 0.6970 - val_sensitivity: 0.5905 - val_iou: 0.4580 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.4030 - dice_coef: 0.5954 - precision: 0.6212 - sensitivity: 0.6166 - iou: 0.4347\n",
      "Epoch 8: val_loss did not improve from 0.37836\n",
      "55/55 [==============================] - 20s 363ms/step - loss: 0.4030 - dice_coef: 0.5954 - precision: 0.6212 - sensitivity: 0.6166 - iou: 0.4347 - val_loss: 0.4187 - val_dice_coef: 0.5813 - val_precision: 0.7336 - val_sensitivity: 0.4972 - val_iou: 0.4213 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.3674 - dice_coef: 0.6297 - precision: 0.6464 - sensitivity: 0.6469 - iou: 0.4714\n",
      "Epoch 9: val_loss improved from 0.37836 to 0.36576, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 371ms/step - loss: 0.3674 - dice_coef: 0.6297 - precision: 0.6464 - sensitivity: 0.6469 - iou: 0.4714 - val_loss: 0.3658 - val_dice_coef: 0.6342 - val_precision: 0.7730 - val_sensitivity: 0.5664 - val_iou: 0.4819 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.3575 - dice_coef: 0.6436 - precision: 0.6782 - sensitivity: 0.6530 - iou: 0.4836\n",
      "Epoch 10: val_loss did not improve from 0.36576\n",
      "55/55 [==============================] - 20s 371ms/step - loss: 0.3575 - dice_coef: 0.6436 - precision: 0.6782 - sensitivity: 0.6530 - iou: 0.4836 - val_loss: 0.3766 - val_dice_coef: 0.6234 - val_precision: 0.6393 - val_sensitivity: 0.6238 - val_iou: 0.4725 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.3285 - dice_coef: 0.6715 - precision: 0.7131 - sensitivity: 0.6639 - iou: 0.5177\n",
      "Epoch 11: val_loss did not improve from 0.36576\n",
      "55/55 [==============================] - 20s 364ms/step - loss: 0.3285 - dice_coef: 0.6715 - precision: 0.7131 - sensitivity: 0.6639 - iou: 0.5177 - val_loss: 0.3939 - val_dice_coef: 0.6061 - val_precision: 0.7576 - val_sensitivity: 0.5252 - val_iou: 0.4526 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.3505 - dice_coef: 0.6472 - precision: 0.6790 - sensitivity: 0.6655 - iou: 0.4887\n",
      "Epoch 12: val_loss improved from 0.36576 to 0.35268, saving model to cxr_reg_full_pixel_unet_breast.best.hdf5\n",
      "55/55 [==============================] - 20s 370ms/step - loss: 0.3505 - dice_coef: 0.6472 - precision: 0.6790 - sensitivity: 0.6655 - iou: 0.4887 - val_loss: 0.3527 - val_dice_coef: 0.6473 - val_precision: 0.7280 - val_sensitivity: 0.6252 - val_iou: 0.4873 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.8977 - dice_coef: 0.1014 - precision: 0.1152 - sensitivity: 0.0963 - iou: 0.0744\n",
      "Epoch 13: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 366ms/step - loss: 0.8977 - dice_coef: 0.1014 - precision: 0.1152 - sensitivity: 0.0963 - iou: 0.0744 - val_loss: 1.0000 - val_dice_coef: 1.7278e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.7278e-05 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.4843e-05 - precision: 0.0000e+00 - sensitivity: 1.0065e-22 - iou: 2.4843e-05\n",
      "Epoch 14: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 365ms/step - loss: 1.0000 - dice_coef: 2.4843e-05 - precision: 0.0000e+00 - sensitivity: 1.0065e-22 - iou: 2.4843e-05 - val_loss: 1.0000 - val_dice_coef: 1.7924e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.7924e-05 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.5354e-05 - precision: 0.0000e+00 - sensitivity: 1.3980e-13 - iou: 2.5354e-05\n",
      "Epoch 15: val_loss did not improve from 0.35268\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "55/55 [==============================] - 20s 370ms/step - loss: 1.0000 - dice_coef: 2.5354e-05 - precision: 0.0000e+00 - sensitivity: 1.3980e-13 - iou: 2.5354e-05 - val_loss: 1.0000 - val_dice_coef: 1.7506e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.7506e-05 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.4416e-05 - precision: 0.0000e+00 - sensitivity: 8.2666e-23 - iou: 2.4416e-05\n",
      "Epoch 16: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 364ms/step - loss: 1.0000 - dice_coef: 2.4416e-05 - precision: 0.0000e+00 - sensitivity: 8.2666e-23 - iou: 2.4416e-05 - val_loss: 1.0000 - val_dice_coef: 1.7682e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.7682e-05 - lr: 5.0000e-05\n",
      "Epoch 17/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.4045e-05 - precision: 0.0000e+00 - sensitivity: 1.9669e-13 - iou: 2.4045e-05\n",
      "Epoch 17: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 363ms/step - loss: 1.0000 - dice_coef: 2.4045e-05 - precision: 0.0000e+00 - sensitivity: 1.9669e-13 - iou: 2.4045e-05 - val_loss: 1.0000 - val_dice_coef: 1.9987e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.9987e-05 - lr: 5.0000e-05\n",
      "Epoch 18/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.6030e-05 - precision: 0.0000e+00 - sensitivity: 3.0342e-22 - iou: 2.6030e-05\n",
      "Epoch 18: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 364ms/step - loss: 1.0000 - dice_coef: 2.6030e-05 - precision: 0.0000e+00 - sensitivity: 3.0342e-22 - iou: 2.6030e-05 - val_loss: 1.0000 - val_dice_coef: 2.0121e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 2.0121e-05 - lr: 5.0000e-05\n",
      "Epoch 19/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.5999e-05 - precision: 0.0000e+00 - sensitivity: 1.6966e-13 - iou: 2.5999e-05\n",
      "Epoch 19: val_loss did not improve from 0.35268\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "55/55 [==============================] - 20s 365ms/step - loss: 1.0000 - dice_coef: 2.5999e-05 - precision: 0.0000e+00 - sensitivity: 1.6966e-13 - iou: 2.5999e-05 - val_loss: 1.0000 - val_dice_coef: 1.8585e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.8585e-05 - lr: 5.0000e-05\n",
      "Epoch 20/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.6680e-05 - precision: 0.0000e+00 - sensitivity: 7.2902e-14 - iou: 2.6680e-05\n",
      "Epoch 20: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 366ms/step - loss: 1.0000 - dice_coef: 2.6680e-05 - precision: 0.0000e+00 - sensitivity: 7.2902e-14 - iou: 2.6680e-05 - val_loss: 1.0000 - val_dice_coef: 2.0280e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 2.0280e-05 - lr: 2.5000e-05\n",
      "Epoch 21/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.3738e-05 - precision: 0.0000e+00 - sensitivity: 1.7138e-22 - iou: 2.3738e-05\n",
      "Epoch 21: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 365ms/step - loss: 1.0000 - dice_coef: 2.3738e-05 - precision: 0.0000e+00 - sensitivity: 1.7138e-22 - iou: 2.3738e-05 - val_loss: 1.0000 - val_dice_coef: 1.9126e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.9126e-05 - lr: 2.5000e-05\n",
      "Epoch 22/50\n",
      "55/55 [==============================] - ETA: 0s - loss: 1.0000 - dice_coef: 2.5063e-05 - precision: 0.0000e+00 - sensitivity: 3.9822e-14 - iou: 2.5063e-05\n",
      "Epoch 22: val_loss did not improve from 0.35268\n",
      "55/55 [==============================] - 20s 362ms/step - loss: 1.0000 - dice_coef: 2.5063e-05 - precision: 0.0000e+00 - sensitivity: 3.9822e-14 - iou: 2.5063e-05 - val_loss: 1.0000 - val_dice_coef: 1.8184e-05 - val_precision: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_iou: 1.8184e-05 - lr: 2.5000e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    epochs=50,\n",
    "    callbacks =callbacks_list\n",
    ")\n",
    "# model.save(\"full_pixel_unet_skin.h5\")\n",
    "model.save(\"full_pixel_unet_breast.h5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T11:31:34.775838765Z",
     "start_time": "2024-01-26T11:24:02.819431991Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "test_images = []\n",
    "ground_truth_test_images = []\n",
    "\n",
    "# test_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/ISIC2018_SKIN_DATA/ISIC2018_Task1-2_Test_Input/*.png\")\n",
    "# test_mask_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/ISIC2018_SKIN_DATA/ISIC2018_Task1_Test_GroundTruth/*.png\")\n",
    "\n",
    "test_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/archive (2)/Dataset_BUSI_with_GT/breast_test/*.png\")\n",
    "test_mask_paths = glob.glob(\"/home/somayeh/PycharmProjects/superpixel_segmentation/archive (2)/Dataset_BUSI_with_GT/breast_test_mask/*.png\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:31:16.636448872Z",
     "start_time": "2024-02-06T10:31:16.593842544Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 28 layers, found 93 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m_full_pixel_unet_breast.best.hdf5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcxr_reg\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/superpixel_segmentation/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/PycharmProjects/superpixel_segmentation/venv/lib/python3.8/site-packages/keras/saving/legacy/hdf5_format.py:812\u001B[0m, in \u001B[0;36mload_weights_from_hdf5_group\u001B[0;34m(f, model)\u001B[0m\n\u001B[1;32m    810\u001B[0m layer_names \u001B[38;5;241m=\u001B[39m filtered_layer_names\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(layer_names) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(filtered_layers):\n\u001B[0;32m--> 812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    813\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLayer count mismatch when loading weights from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel expected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(filtered_layers)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m layers, found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(layer_names)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m saved layers.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    816\u001B[0m     )\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# We batch weight value assignments in a single backend call\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;66;03m# which provides a speedup in TensorFlow.\u001B[39;00m\n\u001B[1;32m    820\u001B[0m weight_value_tuples \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mValueError\u001B[0m: Layer count mismatch when loading weights from file. Model expected 28 layers, found 93 saved layers."
     ]
    }
   ],
   "source": [
    "model.load_weights(\"{}_full_pixel_unet_breast.best.hdf5\".format('cxr_reg'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:31:20.487933461Z",
     "start_time": "2024-02-06T10:31:20.416869561Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 128, 128, 3)\n",
      "(69, 128, 128)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 28 layers, found 93 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 32\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(ground_truth_test_images\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# add a channel dimension to the masks\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# ground_truth_test_images = np.expand_dims(ground_truth_test_images, axis=-1)\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \n\u001B[1;32m     30\u001B[0m \n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# model.load_weights(\"{}_full_pixel_unet_skin.best.hdf5\".format('cxr_reg'))\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m_full_pixel_unet_breast.best.hdf5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcxr_reg\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[1;32m     35\u001B[0m tf\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mrun_functions_eagerly(\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/PycharmProjects/superpixel_segmentation/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/PycharmProjects/superpixel_segmentation/venv/lib/python3.8/site-packages/keras/saving/legacy/hdf5_format.py:812\u001B[0m, in \u001B[0;36mload_weights_from_hdf5_group\u001B[0;34m(f, model)\u001B[0m\n\u001B[1;32m    810\u001B[0m layer_names \u001B[38;5;241m=\u001B[39m filtered_layer_names\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(layer_names) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(filtered_layers):\n\u001B[0;32m--> 812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    813\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLayer count mismatch when loading weights from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel expected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(filtered_layers)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m layers, found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(layer_names)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m saved layers.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    816\u001B[0m     )\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# We batch weight value assignments in a single backend call\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;66;03m# which provides a speedup in TensorFlow.\u001B[39;00m\n\u001B[1;32m    820\u001B[0m weight_value_tuples \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mValueError\u001B[0m: Layer count mismatch when loading weights from file. Model expected 28 layers, found 93 saved layers."
     ]
    }
   ],
   "source": [
    "dice_scores = []\n",
    "for test_path, test_mask_path in zip(test_paths, test_mask_paths):\n",
    "    # read the image and mask using OpenCV\n",
    "    test_image = cv2.imread(test_path)\n",
    "    ground_truth_test_image = cv2.imread(test_mask_path, 0)  # read the mask as grayscale\n",
    "\n",
    "    # resize the image and mask to the desired dimensions\n",
    "    test_image = cv2.resize(test_image, (128, 128))\n",
    "    ground_truth_test_image = cv2.resize(ground_truth_test_image, (128, 128))\n",
    "\n",
    "# normalize the image and mask to have values between 0 and 1\n",
    "\n",
    "    test_image = test_image / 255.0\n",
    "    ground_truth_test_image= ground_truth_test_image / 255.0\n",
    "\n",
    "    # add the image and mask to the corresponding arrays\n",
    "    test_images.append(test_image)\n",
    "    ground_truth_test_images.append(ground_truth_test_image)\n",
    "\n",
    "\n",
    "#by the end of this line, I have superpixeled images.\n",
    "# convert the images and masks to numpy arrays\n",
    "test_images = np.array(test_images)\n",
    "ground_truth_test_images = np.array(ground_truth_test_images)\n",
    "print(test_images.shape)\n",
    "print(ground_truth_test_images.shape)\n",
    "# add a channel dimension to the masks\n",
    "# ground_truth_test_images = np.expand_dims(ground_truth_test_images, axis=-1)\n",
    "\n",
    "\n",
    "# model.load_weights(\"{}_full_pixel_unet_skin.best.hdf5\".format('cxr_reg'))\n",
    "model.load_weights(\"{}_full_pixel_unet_breast.best.hdf5\".format('cxr_reg'))\n",
    "\n",
    "# Make predictions\n",
    "tf.config.run_functions_eagerly(True)\n",
    "predicted_masks = model.predict(\n",
    "    test_images,\n",
    "    batch_size=1,\n",
    "    verbose=1,\n",
    "    steps=None,\n",
    "    callbacks=None\n",
    ")\n",
    "\n",
    "predicted_mask_array = np.array(predicted_masks)\n",
    "print(predicted_mask_array.shape)\n",
    "\n",
    "# Convert the NumPy array to uint8 format\n",
    "predicted_mask_array = (predicted_mask_array * 255).astype(np.uint8)\n",
    "print(predicted_mask_array.shape)\n",
    "\n",
    "#\n",
    "dst = np.empty([predicted_mask_array.shape[0],predicted_mask_array.shape[1],predicted_mask_array.shape[2]])\n",
    "kernel = np.ones((5,5),np.float64)/25\n",
    "# dst = predicted_mask_array\n",
    "\n",
    "#\n",
    "for i in range(predicted_mask_array.shape[0]):\n",
    "\n",
    "        threshold_value = 1\n",
    "        _, dst[i] = cv2.threshold(predicted_mask_array[i], threshold_value, 255, cv2.THRESH_BINARY)\n",
    "        dst[i] = np.array(dst[i])\n",
    "        dst[i] = tf.convert_to_tensor(dst[i] / 255.0,dtype=predicted_masks.dtype)\n",
    "\n",
    "print(dst.shape)\n",
    "\n",
    "## Cast the input tensor to a double tensor\n",
    "ground_truth_test_images = tf.cast(ground_truth_test_images, dtype=tf.float64)\n",
    "predicted_masks = tf.cast(dst, dtype=tf.float64)\n",
    "# predicted_masks = tf.cast(predicted_mask_array, dtype=tf.float64)\n",
    "# Apply thresholding\n",
    "\n",
    "\n",
    "\n",
    "precision_scores = []\n",
    "sensitivity_scores = []\n",
    "iou_scores = []\n",
    "\n",
    "# Iterate over the images and calculate the dice coefficient\n",
    "for i in range(len(ground_truth_test_images)):\n",
    "    y_true = ground_truth_test_images[i]\n",
    "    y_pred = predicted_masks[i]\n",
    "\n",
    "    dice = dice_coef(y_true, y_pred)\n",
    "    dice_scores.append(dice)\n",
    "\n",
    "# Compute the average dice coefficient\n",
    "average_dice = np.mean(dice_scores)\n",
    "\n",
    "for i in range(len(ground_truth_test_images)):\n",
    "    y_true = ground_truth_test_images[i]\n",
    "    y_pred = predicted_masks[i]\n",
    "\n",
    "    precision_value = precision(y_true, y_pred)\n",
    "    precision_scores.append(precision_value)\n",
    "\n",
    "# Compute the average dice coefficient\n",
    "average_precision = np.mean(precision_scores)\n",
    "\n",
    "for i in range(len(ground_truth_test_images)):\n",
    "    y_true = ground_truth_test_images[i]\n",
    "    y_pred = predicted_masks[i]\n",
    "\n",
    "    sensitivity_value = sensitivity(y_true, y_pred)\n",
    "    sensitivity_scores.append(sensitivity_value)\n",
    "\n",
    "# Compute the average dice coefficient\n",
    "average_sensitivity = np.mean(sensitivity_scores)\n",
    "\n",
    "for i in range(len(ground_truth_test_images)):\n",
    "    y_true = ground_truth_test_images[i]\n",
    "    y_pred = predicted_masks[i]\n",
    "\n",
    "    iou_value = iou(y_true, y_pred)\n",
    "    iou_scores.append(iou_value)\n",
    "\n",
    "# Compute the average dice coefficient\n",
    "average_iou = np.mean(iou_scores)\n",
    "\n",
    "# dice_score = dice_coef(ground_truth_test_images, dst)\n",
    "# precision_val = precision(ground_truth_test_images, dst)\n",
    "# sensitivity_val = sensitivity(ground_truth_test_images, dst)\n",
    "# iou_val = iou(ground_truth_test_images, dst)\n",
    "# print(type(dst))\n",
    "\n",
    "print(\"Dices:\", dice_scores)\n",
    "print(\"Dice coefficient:\", average_dice)\n",
    "print(\"Precision:\", average_precision)\n",
    "print(\"Sensitivity:\", average_sensitivity)\n",
    "print(\"IoU:\", average_iou)\n",
    "\n",
    "n =35 # number of images to show\n",
    "for i in range(n):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    ax[0].imshow(test_images[i])\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[1].imshow(ground_truth_test_images[i])\n",
    "    ax[1].set_title('Original Mask')\n",
    "    ax[2].imshow(dst[i])\n",
    "    ax[2].set_title('Predicted Mask')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T10:27:27.612413260Z",
     "start_time": "2024-02-06T10:27:25.945578613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
