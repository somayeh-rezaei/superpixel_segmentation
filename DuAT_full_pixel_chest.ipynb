{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:01:33.044266571Z",
     "start_time": "2024-02-06T13:01:28.817778182Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from pvtv2 import pvt_v2_b2\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import torch\n",
    "from torch import nn\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# image_paths = glob.glob('/home/somayeh/PycharmProjects/superpixel_segmentation/archive (3)/data/Lung Segmentation/CXR_png/*.png' )\n",
    "# mask_paths = glob.glob('/home/somayeh/PycharmProjects/superpixel_segmentation/archive (3)/data/Lung Segmentation/masks/*.png')\n",
    "image_paths = glob.glob('/home/somayeh/PycharmProjects/superpixel_segmentation/new_data_breast/train/image/*.jpg')\n",
    "mask_paths = glob.glob('/home/somayeh/PycharmProjects/superpixel_segmentation/new_data_breast/train/mask/*.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:01:36.511750772Z",
     "start_time": "2024-02-06T13:01:36.325377011Z"
    }
   },
   "id": "288cf8d94e7c9fb8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 12948\n",
      "Number of masks: 12948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the number of images and masks\n",
    "print(f'Number of images: {len(image_paths)}')\n",
    "print(f'Number of masks: {len(mask_paths)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:01:37.448761764Z",
     "start_time": "2024-02-06T13:01:37.440160134Z"
    }
   },
   "id": "21f44483adc50384",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>.\n",
      "macs: 5.233525437\n",
      "params: 24.950883\n"
     ]
    }
   ],
   "source": [
    "from mmengine.model import kaiming_init, constant_init\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Sequential):\n",
    "    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(Block, self).__init__()\n",
    "        if bn_start:\n",
    "            self.add_module('norm1', norm_layer(input_num)),\n",
    "\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n",
    "\n",
    "        self.add_module('norm2', norm_layer(num1)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n",
    "                                            dilation=dilation_rate, padding=dilation_rate)),\n",
    "        self.drop_rate = drop_out\n",
    "\n",
    "    def forward(self, _input):\n",
    "        feature = super(Block, self).forward(_input)\n",
    "        if self.drop_rate > 0:\n",
    "            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n",
    "        return feature\n",
    "    \n",
    "def Upsample(x, size, align_corners = False):\n",
    "    \"\"\"\n",
    "    Wrapper Around the Upsample Call\n",
    "    \"\"\"\n",
    "    return nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=align_corners)\n",
    "\n",
    "    \n",
    "def last_zero_init(m):\n",
    "    if isinstance(m, nn.Sequential):\n",
    "        constant_init(m[-1], val=0)\n",
    "    else:\n",
    "        constant_init(m, val=0)\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_mul', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.pooling_type == 'att':\n",
    "            kaiming_init(self.conv_mask, mode='fan_in')\n",
    "            self.conv_mask.inited = True\n",
    "\n",
    "        if self.channel_add_conv is not None:\n",
    "            last_zero_init(self.channel_add_conv)\n",
    "        if self.channel_mul_conv is not None:\n",
    "            last_zero_init(self.channel_mul_conv)\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out + out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "       \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ca = ChannelAttention(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        res2 = x\n",
    "        x = self.conv1(x)        \n",
    "        x = x + self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x_mask = self.sigmoid_spatial(x)\n",
    "        res1 = res1 * x_mask\n",
    "        return res2 + res1\n",
    "    \n",
    "class GLSA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, embed_dim=32, k_s=3):\n",
    "        super().__init__()\n",
    "                      \n",
    "        self.conv1_1 = BasicConv2d(embed_dim*2,embed_dim, 1)\n",
    "        self.conv1_1_1 = BasicConv2d(input_dim//2,embed_dim,1)\n",
    "        self.local_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.global_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.GlobelBlock = ContextBlock(inplanes= embed_dim, ratio=2)\n",
    "        self.local = ConvBranch(in_features = embed_dim, hidden_features = embed_dim, out_features = embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x_0, x_1 = x.chunk(2,dim = 1)  \n",
    "        \n",
    "    # local block \n",
    "        local = self.local(self.local_11conv(x_0))\n",
    "        \n",
    "    # Globel block    \n",
    "        Globel = self.GlobelBlock(self.global_11conv(x_1))\n",
    "\n",
    "    # concat Globel + local\n",
    "        x = torch.cat([local,Globel], dim=1)\n",
    "        x = self.conv1_1(x)\n",
    "\n",
    "        return x    \n",
    "    \n",
    "class SBA(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.d_in1 = BasicConv2d(input_dim//2, input_dim//2, 1)\n",
    "        self.d_in2 = BasicConv2d(input_dim//2, input_dim//2, 1)       \n",
    "                \n",
    "\n",
    "        self.conv = nn.Sequential(BasicConv2d(input_dim, input_dim, 3,1,1), nn.Conv2d(input_dim, 1, kernel_size=1, bias=False))\n",
    "        self.fc1 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, H_feature, L_feature):\n",
    "\n",
    "        L_feature = self.fc1(L_feature)\n",
    "        H_feature = self.fc2(H_feature)\n",
    "        \n",
    "        g_L_feature =  self.Sigmoid(L_feature)\n",
    "        g_H_feature = self.Sigmoid(H_feature)\n",
    "        \n",
    "        L_feature = self.d_in1(L_feature)\n",
    "        H_feature = self.d_in2(H_feature)\n",
    "\n",
    "\n",
    "        L_feature = L_feature + L_feature * g_L_feature + (1 - g_L_feature) * Upsample(g_H_feature * H_feature, size= L_feature.size()[2:], align_corners=False)\n",
    "        H_feature = H_feature + H_feature * g_H_feature + (1 - g_H_feature) * Upsample(g_L_feature * L_feature, size= H_feature.size()[2:], align_corners=False) \n",
    "        \n",
    "        H_feature = Upsample(H_feature, size = L_feature.size()[2:])\n",
    "        out = self.conv(torch.cat([H_feature,L_feature], dim=1))\n",
    "        return out\n",
    "\n",
    "class DuAT(nn.Module):\n",
    "    def __init__(self, dim=32, dims= [64, 128, 320, 512]):\n",
    "        super(DuAT, self).__init__()\n",
    "\n",
    "        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n",
    "        path = './pretrained_pth/pvt_v2_b2.pth'\n",
    "        save_model = torch.load(path)\n",
    "        model_dict = self.backbone.state_dict()\n",
    "        state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n",
    "        model_dict.update(state_dict)\n",
    "        self.backbone.load_state_dict(model_dict)\n",
    "\n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = dims[0], dims[1], dims[2], dims[3]\n",
    "\n",
    "        self.GLSA_c4 = GLSA(input_dim=c4_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c3 = GLSA(input_dim=c3_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c2 = GLSA(input_dim=c2_in_channels, embed_dim=dim)\n",
    "        self.L_feature = BasicConv2d(c1_in_channels,dim, 3,1,1)\n",
    "        \n",
    "        self.SBA = SBA(input_dim = dim)\n",
    "        self.fuse = BasicConv2d(dim * 2, dim, 1)\n",
    "        self.fuse2 = nn.Sequential(BasicConv2d(dim*3, dim, 1,1),nn.Conv2d(dim, 1, kernel_size=1, bias=False))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # backbone\n",
    "        pvt = self.backbone(x)\n",
    "        c1, c2, c3, c4 = pvt\n",
    "        n, _, h, w = c4.shape        \n",
    "        _c4 = self.GLSA_c4(c4) # [1, 64, 11, 11]\n",
    "        _c4 = Upsample(_c4, c3.size()[2:])\n",
    "        _c3 = self.GLSA_c3(c3) # [1, 64, 22, 22]\n",
    "        _c2 = self.GLSA_c2(c2) # [1, 64, 44, 44]\n",
    "        \n",
    "        output = self.fuse2(torch.cat([Upsample(_c4, c2.size()[2:]), Upsample(_c3, c2.size()[2:]), _c2], dim=1))\n",
    "        \n",
    "        L_feature = self.L_feature(c1)  # [1, 64, 88, 88]\n",
    "        H_feature = self.fuse(torch.cat([_c4, _c3], dim=1))\n",
    "        H_feature = Upsample(H_feature,c2.size()[2:])\n",
    "        \n",
    "        output2 = self.SBA(H_feature,L_feature)\n",
    "        \n",
    "        output = F.interpolate(output, scale_factor=8, mode='bilinear')\n",
    "        output2 = F.interpolate(output2, scale_factor=4, mode='bilinear')\n",
    "        \n",
    "        return output, output2\n",
    "\n",
    "\n",
    "model = DuAT()\n",
    "from torchinfo import summary\n",
    "#   summary(model, (1, 3, 352, 352))\n",
    "from thop import profile\n",
    "input = torch.randn(1, 3, 256, 256)\n",
    "macs, params = profile(model, inputs=(input,))\n",
    "print('macs:', macs / 1000000000)\n",
    "print('params:', params / 1000000)    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:01:42.107452579Z",
     "start_time": "2024-02-06T13:01:38.975621179Z"
    }
   },
   "id": "fc54dcb8f17b62d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12948/12948 [00:39<00:00, 324.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for images and masks\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Lists to hold processed images and masks\n",
    "processed_images = []\n",
    "processed_masks = []\n",
    "\n",
    "for img_path, mask_path in tqdm(zip(image_paths, mask_paths), total=len(image_paths)):\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Apply transformations\n",
    "    image = transform(Image.fromarray(image))\n",
    "    mask = transform(Image.fromarray(mask))\n",
    "\n",
    "    processed_images.append(image)\n",
    "    processed_masks.append(mask)\n",
    "# Convert the lists of tensors to a single tensor\n",
    "images_tensor = torch.stack(processed_images)\n",
    "masks_tensor = torch.stack(processed_masks)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets (70%, 20%, 10%)\n",
    "images_train, images_temp, masks_train, masks_temp = train_test_split(images_tensor, masks_tensor, test_size=0.3, random_state=42)\n",
    "images_val, images_test, masks_val, masks_test = train_test_split(images_temp, masks_temp, test_size=2/3, random_state=2023)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 8\n",
    "# Train DataLoader\n",
    "train_dataset = TensorDataset(images_train, masks_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_dataset = TensorDataset(images_val, masks_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test DataLoader\n",
    "test_dataset = TensorDataset(images_test, masks_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:02:26.870068282Z",
     "start_time": "2024-02-06T13:01:43.278838972Z"
    }
   },
   "id": "3b23a897ec027e15"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_coefficient(pred, target):\n",
    "    smooth = 1e-6\n",
    "    pred = pred.gt(0.5)  # Threshold to convert to boolean\n",
    "    target = target.gt(0.5)\n",
    "    batch_size = pred.size(0)\n",
    "    pred = pred.view(batch_size, -1)\n",
    "    target = target.view(batch_size, -1)\n",
    "    intersection = (pred & target).sum(1)\n",
    "    dice = (2. * intersection.float() + smooth) / (pred.sum(1) + target.sum(1) + smooth)\n",
    "    return dice.mean().item()\n",
    "\n",
    "def iou(pred, target):\n",
    "    pred = pred.gt(0.5)  # Threshold to convert to boolean\n",
    "    target = target.gt(0.5)\n",
    "    batch_size = pred.size(0)\n",
    "    pred = pred.view(batch_size, -1)\n",
    "    target = target.view(batch_size, -1)\n",
    "    intersection = (pred & target).sum(1).float()\n",
    "    union = (pred | target).sum(1).float()\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    return iou.mean().item()\n",
    "\n",
    "def precision(pred, target):\n",
    "    pred = pred.gt(0.5)  # Threshold to convert to boolean\n",
    "    target = target.gt(0.5)\n",
    "    batch_size = pred.size(0)\n",
    "    pred = pred.view(batch_size, -1)\n",
    "    target = target.view(batch_size, -1)\n",
    "    true_positives = (pred & target).sum(1).float()\n",
    "    all_positives = pred.sum(1).float()\n",
    "    precision = true_positives / (all_positives + 1e-8)\n",
    "    return precision.mean().item()\n",
    "\n",
    "def recall(pred, target):\n",
    "    pred = pred.gt(0.5)  # Threshold to convert to boolean\n",
    "    target = target.gt(0.5)\n",
    "    batch_size = pred.size(0)\n",
    "    pred = pred.view(batch_size, -1)\n",
    "    target = target.view(batch_size, -1)\n",
    "    true_positives = (pred & target).sum(1).float()\n",
    "    actual_positives = target.sum(1).float()\n",
    "    recall = true_positives / (actual_positives + 1e-8)\n",
    "    return recall.mean().item()\n",
    "\n",
    "# Define the Dice Loss function\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, masks):\n",
    "        smooth = 1e-6  # Smoothing value to avoid division by zero\n",
    "        batch_size = outputs.size(0)\n",
    "\n",
    "        outputs = outputs.view(batch_size, -1)\n",
    "        masks = masks.view(batch_size, -1)\n",
    "        # print(\"Shapes - Outputs:\", outputs.size(), \"Masks:\", masks.size())\n",
    "\n",
    "        intersection = (outputs * masks).sum(1)\n",
    "        union = outputs.sum(1) + masks.sum(1)\n",
    "\n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "        dice_loss = 1 - dice.mean()\n",
    "\n",
    "        return dice_loss\n",
    "    \n",
    "def structure_loss(pred, target):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(target, kernel_size=31, stride=1, padding=15) - target)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * target) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + target) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:02:33.962462349Z",
     "start_time": "2024-02-06T13:02:33.956619803Z"
    }
   },
   "id": "2dff751666cdce79"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [1/50] - Loss: 1.3450, Dice: 0.4737, IoU: 0.3582, Precision: 0.3759, Recall: 0.8911\n",
      "Validation Results: Loss: 1.2152, Dice: 0.6326, IoU: 0.5085, Precision: 0.5360, Recall: 0.8826\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [2/50] - Loss: 1.1867, Dice: 0.6574, IoU: 0.5377, Precision: 0.5689, Recall: 0.8833\n",
      "Validation Results: Loss: 1.1386, Dice: 0.6937, IoU: 0.5754, Precision: 0.6063, Recall: 0.8827\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [3/50] - Loss: 1.1160, Dice: 0.7118, IoU: 0.5953, Precision: 0.6285, Recall: 0.8880\n",
      "Validation Results: Loss: 1.0844, Dice: 0.7394, IoU: 0.6263, Precision: 0.6643, Recall: 0.8776\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [4/50] - Loss: 1.0604, Dice: 0.7409, IoU: 0.6285, Precision: 0.6616, Recall: 0.8948\n",
      "Validation Results: Loss: 1.0390, Dice: 0.7336, IoU: 0.6190, Precision: 0.6380, Recall: 0.9143\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [5/50] - Loss: 1.0075, Dice: 0.7621, IoU: 0.6527, Precision: 0.6848, Recall: 0.9019\n",
      "Validation Results: Loss: 0.9883, Dice: 0.7766, IoU: 0.6664, Precision: 0.7006, Recall: 0.8896\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [6/50] - Loss: 0.9571, Dice: 0.7802, IoU: 0.6730, Precision: 0.7052, Recall: 0.9053\n",
      "Validation Results: Loss: 0.9290, Dice: 0.7869, IoU: 0.6853, Precision: 0.7165, Recall: 0.8991\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [7/50] - Loss: 0.9113, Dice: 0.7934, IoU: 0.6887, Precision: 0.7199, Recall: 0.9095\n",
      "Validation Results: Loss: 0.8910, Dice: 0.7979, IoU: 0.6887, Precision: 0.7233, Recall: 0.8928\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [8/50] - Loss: 0.8648, Dice: 0.8099, IoU: 0.7044, Precision: 0.7341, Recall: 0.9134\n",
      "Validation Results: Loss: 0.8510, Dice: 0.8090, IoU: 0.7038, Precision: 0.7383, Recall: 0.8937\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [9/50] - Loss: 0.8230, Dice: 0.8147, IoU: 0.7143, Precision: 0.7451, Recall: 0.9147\n",
      "Validation Results: Loss: 0.8034, Dice: 0.8075, IoU: 0.7031, Precision: 0.7286, Recall: 0.9109\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [10/50] - Loss: 0.7799, Dice: 0.8281, IoU: 0.7275, Precision: 0.7568, Recall: 0.9174\n",
      "Validation Results: Loss: 0.7710, Dice: 0.8195, IoU: 0.7188, Precision: 0.7506, Recall: 0.9009\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [11/50] - Loss: 0.7409, Dice: 0.8356, IoU: 0.7350, Precision: 0.7647, Recall: 0.9181\n",
      "Validation Results: Loss: 0.7360, Dice: 0.8331, IoU: 0.7343, Precision: 0.7715, Recall: 0.8944\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [12/50] - Loss: 0.7006, Dice: 0.8428, IoU: 0.7459, Precision: 0.7752, Recall: 0.9203\n",
      "Validation Results: Loss: 0.7125, Dice: 0.8303, IoU: 0.7349, Precision: 0.7642, Recall: 0.9052\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [13/50] - Loss: 0.6633, Dice: 0.8500, IoU: 0.7541, Precision: 0.7835, Recall: 0.9208\n",
      "Validation Results: Loss: 0.6485, Dice: 0.8393, IoU: 0.7412, Precision: 0.7697, Recall: 0.9059\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [14/50] - Loss: 0.6274, Dice: 0.8553, IoU: 0.7609, Precision: 0.7905, Recall: 0.9209\n",
      "Validation Results: Loss: 0.6231, Dice: 0.8461, IoU: 0.7511, Precision: 0.7846, Recall: 0.9000\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [15/50] - Loss: 0.5917, Dice: 0.8622, IoU: 0.7693, Precision: 0.7986, Recall: 0.9217\n",
      "Validation Results: Loss: 0.5960, Dice: 0.8546, IoU: 0.7621, Precision: 0.8014, Recall: 0.8924\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [16/50] - Loss: 0.5572, Dice: 0.8689, IoU: 0.7770, Precision: 0.8067, Recall: 0.9230\n",
      "Validation Results: Loss: 0.5585, Dice: 0.8526, IoU: 0.7597, Precision: 0.7882, Recall: 0.9061\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [17/50] - Loss: 0.5258, Dice: 0.8735, IoU: 0.7830, Precision: 0.8132, Recall: 0.9225\n",
      "Validation Results: Loss: 0.5215, Dice: 0.8597, IoU: 0.7707, Precision: 0.8044, Recall: 0.9023\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [18/50] - Loss: 0.4975, Dice: 0.8789, IoU: 0.7875, Precision: 0.8181, Recall: 0.9219\n",
      "Validation Results: Loss: 0.4887, Dice: 0.8667, IoU: 0.7786, Precision: 0.8226, Recall: 0.8879\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [19/50] - Loss: 0.4665, Dice: 0.8862, IoU: 0.7943, Precision: 0.8249, Recall: 0.9229\n",
      "Validation Results: Loss: 0.4683, Dice: 0.8723, IoU: 0.7806, Precision: 0.8229, Recall: 0.8897\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [20/50] - Loss: 0.4388, Dice: 0.8920, IoU: 0.8003, Precision: 0.8311, Recall: 0.9231\n",
      "Validation Results: Loss: 0.4438, Dice: 0.8682, IoU: 0.7803, Precision: 0.8163, Recall: 0.8983\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [21/50] - Loss: 0.4173, Dice: 0.8914, IoU: 0.8045, Precision: 0.8362, Recall: 0.9221\n",
      "Validation Results: Loss: 0.4483, Dice: 0.8699, IoU: 0.7814, Precision: 0.8159, Recall: 0.9005\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [22/50] - Loss: 0.3917, Dice: 0.8969, IoU: 0.8100, Precision: 0.8413, Recall: 0.9232\n",
      "Validation Results: Loss: 0.4238, Dice: 0.8712, IoU: 0.7825, Precision: 0.8242, Recall: 0.8922\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [23/50] - Loss: 0.3717, Dice: 0.9007, IoU: 0.8141, Precision: 0.8456, Recall: 0.9236\n",
      "Validation Results: Loss: 0.4042, Dice: 0.8712, IoU: 0.7886, Precision: 0.8260, Recall: 0.8992\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [24/50] - Loss: 0.3512, Dice: 0.9034, IoU: 0.8184, Precision: 0.8500, Recall: 0.9233\n",
      "Validation Results: Loss: 0.3953, Dice: 0.8785, IoU: 0.7938, Precision: 0.8334, Recall: 0.8951\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [25/50] - Loss: 0.3327, Dice: 0.9093, IoU: 0.8233, Precision: 0.8546, Recall: 0.9245\n",
      "Validation Results: Loss: 0.3651, Dice: 0.8834, IoU: 0.8011, Precision: 0.8411, Recall: 0.8953\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [26/50] - Loss: 0.3175, Dice: 0.9112, IoU: 0.8266, Precision: 0.8579, Recall: 0.9242\n",
      "Validation Results: Loss: 0.3624, Dice: 0.8802, IoU: 0.8001, Precision: 0.8356, Recall: 0.9010\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [27/50] - Loss: 0.3038, Dice: 0.9127, IoU: 0.8298, Precision: 0.8614, Recall: 0.9242\n",
      "Validation Results: Loss: 0.3454, Dice: 0.8827, IoU: 0.7967, Precision: 0.8305, Recall: 0.9029\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [28/50] - Loss: 0.2899, Dice: 0.9159, IoU: 0.8334, Precision: 0.8648, Recall: 0.9246\n",
      "Validation Results: Loss: 0.3293, Dice: 0.8878, IoU: 0.8052, Precision: 0.8400, Recall: 0.9041\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [29/50] - Loss: 0.2792, Dice: 0.9153, IoU: 0.8360, Precision: 0.8669, Recall: 0.9255\n",
      "Validation Results: Loss: 0.3282, Dice: 0.8869, IoU: 0.8065, Precision: 0.8484, Recall: 0.8949\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [30/50] - Loss: 0.2667, Dice: 0.9201, IoU: 0.8396, Precision: 0.8704, Recall: 0.9257\n",
      "Validation Results: Loss: 0.3142, Dice: 0.8913, IoU: 0.8141, Precision: 0.8559, Recall: 0.8949\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [31/50] - Loss: 0.2582, Dice: 0.9215, IoU: 0.8420, Precision: 0.8729, Recall: 0.9261\n",
      "Validation Results: Loss: 0.3091, Dice: 0.8923, IoU: 0.8133, Precision: 0.8532, Recall: 0.8946\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [32/50] - Loss: 0.2489, Dice: 0.9233, IoU: 0.8451, Precision: 0.8755, Recall: 0.9269\n",
      "Validation Results: Loss: 0.3011, Dice: 0.8958, IoU: 0.8159, Precision: 0.8546, Recall: 0.8978\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [33/50] - Loss: 0.2430, Dice: 0.9236, IoU: 0.8464, Precision: 0.8770, Recall: 0.9270\n",
      "Validation Results: Loss: 0.2967, Dice: 0.8970, IoU: 0.8172, Precision: 0.8578, Recall: 0.8972\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [34/50] - Loss: 0.2313, Dice: 0.9286, IoU: 0.8513, Precision: 0.8809, Recall: 0.9284\n",
      "Validation Results: Loss: 0.2896, Dice: 0.8976, IoU: 0.8220, Precision: 0.8631, Recall: 0.8981\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [35/50] - Loss: 0.2258, Dice: 0.9307, IoU: 0.8527, Precision: 0.8822, Recall: 0.9291\n",
      "Validation Results: Loss: 0.2917, Dice: 0.8970, IoU: 0.8228, Precision: 0.8633, Recall: 0.8984\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [36/50] - Loss: 0.2198, Dice: 0.9313, IoU: 0.8548, Precision: 0.8841, Recall: 0.9295\n",
      "Validation Results: Loss: 0.2896, Dice: 0.9001, IoU: 0.8216, Precision: 0.8644, Recall: 0.8954\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [37/50] - Loss: 0.2154, Dice: 0.9323, IoU: 0.8561, Precision: 0.8854, Recall: 0.9296\n",
      "Validation Results: Loss: 0.2826, Dice: 0.9021, IoU: 0.8249, Precision: 0.8690, Recall: 0.8930\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [38/50] - Loss: 0.2108, Dice: 0.9323, IoU: 0.8581, Precision: 0.8871, Recall: 0.9302\n",
      "Validation Results: Loss: 0.2795, Dice: 0.8953, IoU: 0.8236, Precision: 0.8606, Recall: 0.9040\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [39/50] - Loss: 0.2074, Dice: 0.9329, IoU: 0.8597, Precision: 0.8885, Recall: 0.9305\n",
      "Validation Results: Loss: 0.2865, Dice: 0.9013, IoU: 0.8238, Precision: 0.8661, Recall: 0.8954\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [40/50] - Loss: 0.2016, Dice: 0.9371, IoU: 0.8615, Precision: 0.8901, Recall: 0.9305\n",
      "Validation Results: Loss: 0.2813, Dice: 0.9022, IoU: 0.8263, Precision: 0.8629, Recall: 0.9035\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [41/50] - Loss: 0.1952, Dice: 0.9346, IoU: 0.8582, Precision: 0.8875, Recall: 0.9295\n",
      "Validation Results: Loss: 0.2711, Dice: 0.9061, IoU: 0.8225, Precision: 0.8557, Recall: 0.9031\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [42/50] - Loss: 0.1807, Dice: 0.9377, IoU: 0.8605, Precision: 0.8888, Recall: 0.9304\n",
      "Validation Results: Loss: 0.2710, Dice: 0.9061, IoU: 0.8270, Precision: 0.8614, Recall: 0.9012\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [43/50] - Loss: 0.1714, Dice: 0.9410, IoU: 0.8638, Precision: 0.8915, Recall: 0.9318\n",
      "Validation Results: Loss: 0.2661, Dice: 0.9097, IoU: 0.8299, Precision: 0.8654, Recall: 0.9001\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [44/50] - Loss: 0.1668, Dice: 0.9419, IoU: 0.8649, Precision: 0.8926, Recall: 0.9321\n",
      "Validation Results: Loss: 0.2697, Dice: 0.9076, IoU: 0.8278, Precision: 0.8604, Recall: 0.9028\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [45/50] - Loss: 0.1659, Dice: 0.9418, IoU: 0.8650, Precision: 0.8926, Recall: 0.9319\n",
      "Validation Results: Loss: 0.2691, Dice: 0.9089, IoU: 0.8264, Precision: 0.8584, Recall: 0.9019\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [46/50] - Loss: 0.1663, Dice: 0.9408, IoU: 0.8638, Precision: 0.8919, Recall: 0.9315\n",
      "Validation Results: Loss: 0.2695, Dice: 0.9050, IoU: 0.8275, Precision: 0.8586, Recall: 0.9049\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [47/50] - Loss: 0.1567, Dice: 0.9448, IoU: 0.8689, Precision: 0.8954, Recall: 0.9336\n",
      "Validation Results: Loss: 0.2663, Dice: 0.9079, IoU: 0.8289, Precision: 0.8647, Recall: 0.8989\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [48/50] - Loss: 0.1637, Dice: 0.9420, IoU: 0.8650, Precision: 0.8932, Recall: 0.9313\n",
      "Validation Results: Loss: 0.2735, Dice: 0.9102, IoU: 0.8285, Precision: 0.8641, Recall: 0.8962\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [49/50] - Loss: 0.1557, Dice: 0.9447, IoU: 0.8694, Precision: 0.8963, Recall: 0.9334\n",
      "Validation Results: Loss: 0.2738, Dice: 0.9103, IoU: 0.8300, Precision: 0.8662, Recall: 0.8990\n",
      "Number of training images: 9063\n",
      "Number of validation images: 1295\n",
      "Epoch [50/50] - Loss: 0.1561, Dice: 0.9436, IoU: 0.8680, Precision: 0.8953, Recall: 0.9329\n",
      "Validation Results: Loss: 0.2775, Dice: 0.9084, IoU: 0.8289, Precision: 0.8616, Recall: 0.9014\n"
     ]
    }
   ],
   "source": [
    "# # Define batch size and whether to shuffle the data\n",
    "# batch_size = 8\n",
    "# shuffle = True\n",
    "# patience = 10\n",
    "# best_val_loss = float('inf')\n",
    "# counter = 0\n",
    "# # Define your model\n",
    "# model = DuAT()  # Replace with your network\n",
    "# \n",
    "# # Set device type (CPU or GPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# \n",
    "# # Move the model and data to the device\n",
    "# model.to(device)\n",
    "# \n",
    "# # Define your loss function and optimizer\n",
    "# criterion = structure_loss # Use the Dice Loss function\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)  # Adjust learning rate and optimizer as needed\n",
    "# \n",
    "# # Your training loop\n",
    "# num_epochs = 50  # Set the number of epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Number of training images: {len(train_loader.dataset)}\")\n",
    "#     print(f\"Number of validation images: {len(val_loader.dataset)}\")\n",
    "#     model.train()\n",
    "#     epoch_loss = 0.0\n",
    "#     epoch_dice = 0.0\n",
    "#     epoch_iou = 0.0\n",
    "#     epoch_precision = 0.0\n",
    "#     epoch_recall = 0.0\n",
    "#     \n",
    "#     for images, masks in train_loader:\n",
    "#         # Move images and masks to the device\n",
    "#         images, masks = images.to(device), masks.to(device)\n",
    "#         \n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "# \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         if isinstance(outputs, tuple):\n",
    "#             outputs = outputs[0]\n",
    "# \n",
    "#         # Calculate the loss using Dice Loss\n",
    "#         loss = criterion(outputs, masks)\n",
    "# \n",
    "#         # Backpropagation\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         # Metrics calculation\n",
    "#         with torch.no_grad():\n",
    "#             pred_masks = (torch.sigmoid(outputs) > 0.5).float()  # Assuming it's a binary mask\n",
    "#             epoch_loss += loss.item() * images.size(0)\n",
    "#             epoch_dice += dice_coefficient(pred_masks, masks)\n",
    "#             epoch_iou += iou(pred_masks, masks)\n",
    "#             epoch_precision += precision(pred_masks, masks)\n",
    "#             epoch_recall += recall(pred_masks, masks)\n",
    "# \n",
    "#     # Calculate average epoch metrics\n",
    "#     epoch_loss /= len(train_loader.dataset)\n",
    "#     epoch_dice /= len(train_loader)\n",
    "#     epoch_iou /= len(train_loader)\n",
    "#     epoch_precision /= len(train_loader)\n",
    "#     epoch_recall /= len(train_loader)\n",
    "#     \n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}, Dice: {epoch_dice:.4f}, IoU: {epoch_iou:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}\")\n",
    "# \n",
    "#     # Validation loop\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_dice = 0.0\n",
    "#     val_iou = 0.0\n",
    "#     val_precision = 0.0\n",
    "#     val_recall = 0.0\n",
    "# \n",
    "#     for images, masks in val_loader:\n",
    "#         images, masks = images.to(device), masks.to(device)\n",
    "#     \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "#             if isinstance(outputs, tuple):\n",
    "#                 outputs = outputs[0]\n",
    "#     \n",
    "#             loss = criterion(outputs, masks)\n",
    "#             pred_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "#     \n",
    "#             val_loss += loss.item() * images.size(0)\n",
    "#             val_dice += dice_coefficient(pred_masks, masks)\n",
    "#             val_iou += iou(pred_masks, masks)\n",
    "#             val_precision += precision(pred_masks, masks)\n",
    "#             val_recall += recall(pred_masks, masks)\n",
    "#     \n",
    "#     val_loss /= len(val_loader.dataset)\n",
    "#     val_dice /= len(val_loader)\n",
    "#     val_iou /= len(val_loader)\n",
    "#     val_precision /= len(val_loader)\n",
    "#     val_recall /= len(val_loader)\n",
    "#     \n",
    "#     print(f\"Validation Results: Loss: {val_loss:.4f}, Dice: {val_dice:.4f}, IoU: {val_iou:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
    "#     \n",
    "#         # Check early stopping conditions\n",
    "#     if val_loss < best_val_loss:\n",
    "#         counter = 0\n",
    "#         best_val_loss = val_loss\n",
    "#         # Save the best model checkpoint\n",
    "#         torch.save(model.state_dict(), 'DuAT_full_pixel_breast.pth')\n",
    "#     else:\n",
    "#         counter += 1\n",
    "#         if counter >= patience:\n",
    "#             print(f'Early stopping at epoch {epoch}...')\n",
    "#             break  # Stop training\n",
    "#     \n",
    "#     # # Save model checkpoint after every epoch\n",
    "#     # torch.save(model.state_dict(), f'checkpoint_epoch_{epoch}.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T11:47:57.999693740Z",
     "start_time": "2024-02-01T09:56:09.869011311Z"
    }
   },
   "id": "ad516cb3f167b43b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DuAT()  # Replace with your network\n",
    "model.load_state_dict(torch.load('DuAT_full_pixel_breast.pth'))  # Load the best model weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:02:49.431676765Z",
     "start_time": "2024-02-06T13:02:42.763349353Z"
    }
   },
   "id": "a3c87989360d1956",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# model = DuAT()  # Replace with your network\n",
    "# model.load_state_dict(torch.load('DuAT_full_pixel_chest.pth'))  # Load the best model weights\n",
    "model.load_state_dict(torch.load('DuAT_full_pixel_breast.pth'))  # Load the best model weights\n",
    "\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model and data to the device\n",
    "model.to(device)\n",
    "images_test = images_test.to(device)\n",
    "masks_test =masks_test.to(device)\n",
    "\n",
    "model.eval()\n",
    "# Prepare a DataLoader for the test dataset\n",
    "test_dataset = TensorDataset(images_test, masks_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Variables to store metrics\n",
    "test_dice = 0.0\n",
    "test_iou = 0.0\n",
    "test_precision = 0.0\n",
    "test_recall = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:02:54.340564782Z",
     "start_time": "2024-02-06T13:02:53.782111765Z"
    }
   },
   "id": "a613a05f72863c23"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# # Iterate through the test dataset\n",
    "# counter = 0\n",
    "# for images, masks in test_loader:\n",
    "#     with torch.no_grad():\n",
    "#         # Get model predictions\n",
    "#         outputs = model(images)\n",
    "#         if isinstance(outputs, tuple):\n",
    "#             outputs = outputs[0]\n",
    "# \n",
    "#         # Calculate the metrics using the defined functions\n",
    "#         test_dice += dice_coefficient(torch.sigmoid(outputs), masks)\n",
    "#         test_iou += iou(torch.sigmoid(outputs), masks)\n",
    "#         test_precision += precision(torch.sigmoid(outputs), masks)\n",
    "#         test_recall += recall(torch.sigmoid(outputs), masks)\n",
    "#         \n",
    "#                 # For visualization of the first 30 images\n",
    "#         for i in range(len(images)):\n",
    "#             counter += 1\n",
    "#             if counter <= 30:\n",
    "#                 # Get the predictions for the current image\n",
    "#                 pred_masks = (torch.sigmoid(outputs[i]) > 0.5).float()\n",
    "# \n",
    "#                 # Plot original image, ground truth mask, and predicted mask\n",
    "#                 fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "#                 axs[0].imshow(images[i].permute(1, 2, 0).cpu().numpy())\n",
    "#                 axs[0].set_title('Original Image')\n",
    "# \n",
    "#                 axs[1].imshow(masks[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "#                 axs[1].set_title('Ground Truth Mask')\n",
    "# \n",
    "#                 axs[2].imshow(pred_masks.squeeze().cpu().numpy(), cmap='gray')\n",
    "#                 axs[2].set_title('Predicted Mask')\n",
    "# \n",
    "#                 plt.show()\n",
    "#         \n",
    "# \n",
    "# # Calculate the average metrics\n",
    "# num_samples = len(test_loader)\n",
    "# test_dice /= num_samples\n",
    "# test_iou /= num_samples\n",
    "# test_precision /= num_samples\n",
    "# test_recall /= num_samples\n",
    "# \n",
    "# print(f\"Test Results - Dice: {test_dice:.4f}, IoU: {test_iou:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}\")\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:02:57.959275343Z",
     "start_time": "2024-02-06T13:02:57.955901878Z"
    }
   },
   "id": "51eb9d07f998064"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "# Define the base directory where you want to save the images\n",
    "base_save_path = \"/home/somayeh/PycharmProjects/superpixel_segmentation/result_images/DuAT_full_pixel_breast/\"\n",
    "\n",
    "# Subdirectories for each image type\n",
    "original_images_dir = os.path.join(base_save_path, \"original_images\")\n",
    "original_masks_dir = os.path.join(base_save_path, \"original_masks\")\n",
    "predicted_masks_dir = os.path.join(base_save_path, \"predicted_masks\")\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "os.makedirs(original_images_dir, exist_ok=True)\n",
    "os.makedirs(original_masks_dir, exist_ok=True)\n",
    "os.makedirs(predicted_masks_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:03:55.250251456Z",
     "start_time": "2024-02-06T13:03:55.207339823Z"
    }
   },
   "id": "d833e413df07d83e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "counter = 0\n",
    "for images, masks in test_loader:\n",
    "    with torch.no_grad():\n",
    "        # Similar preprocessing as before\n",
    "        outputs = model(images)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "\n",
    "                # Calculate the metrics using the defined functions\n",
    "        test_dice += dice_coefficient(torch.sigmoid(outputs), masks)\n",
    "        test_iou += iou(torch.sigmoid(outputs), masks)\n",
    "        test_precision += precision(torch.sigmoid(outputs), masks)\n",
    "        test_recall += recall(torch.sigmoid(outputs), masks)\n",
    "\n",
    "        # Visualization with OpenCV\n",
    "        for i in range(len(images)):\n",
    "            counter += 1\n",
    "            if counter <= 30:\n",
    "                pred_masks = (torch.sigmoid(outputs[i]) > 0.5).float()\n",
    "                original_image = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "                ground_truth_mask = masks[i].squeeze().cpu().numpy()\n",
    "                predicted_mask = pred_masks.squeeze().cpu().numpy()\n",
    "\n",
    "                # Convert the original image from RGB to BGR for OpenCV display\n",
    "                original_image_bgr = cv2.cvtColor(np.float32(original_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Normalize the original image for display\n",
    "                original_image_bgr = cv2.normalize(original_image_bgr, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "                # Display each image separately\n",
    "                # cv2.imshow('Original Image', original_image_bgr)\n",
    "                # cv2.imshow('Ground Truth Mask', ground_truth_mask)\n",
    "                # cv2.imshow('Predicted Mask', predicted_mask)\n",
    "\n",
    "                # Wait for a key press to continue to the next set of images\n",
    "                # cv2.waitKey(0)\n",
    "                \n",
    "                # Define file paths for saving\n",
    "                original_image_path = os.path.join(original_images_dir, f\"original_image_{counter}.png\")\n",
    "                ground_truth_mask_path = os.path.join(original_masks_dir, f\"ground_truth_mask_{counter}.png\")\n",
    "                predicted_mask_path = os.path.join(predicted_masks_dir, f\"predicted_mask_{counter}.png\")\n",
    "\n",
    "                # Save the images\n",
    "                cv2.imwrite(original_image_path, original_image_bgr)\n",
    "                cv2.imwrite(ground_truth_mask_path, ground_truth_mask * 255)  # Ensure mask is in 0-255 range\n",
    "                cv2.imwrite(predicted_mask_path, predicted_mask * 255)  # Ensure mask is in 0-255 range\n",
    "\n",
    "                \n",
    "\n",
    "# cv2.destroyAllWindows()  # Make sure to close the windows after the loop\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T13:04:17.147585950Z",
     "start_time": "2024-02-06T13:04:06.071492794Z"
    }
   },
   "id": "e1b885c3fb30c68b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8849a3bf5b8fa23c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
